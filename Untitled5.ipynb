{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTxekXHRnkDktOAPnSoD4g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAICHANDUALURI/KDM-ICP-5/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uuf7b_KdCPAA",
        "outputId": "6b9ed23b-641a-4d0a-8cda-7a35d6af40b1"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 68kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 53.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=5c786514018e81585cbd9111a8272411263515b0117a9fbfa810c17e57b84997\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JWonTdMFiYq"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU6EfWjTGCxo"
      },
      "source": [
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzijq4m2GWiy"
      },
      "source": [
        "# creating spark dataframe wiht the input data. You can also read the data from file. label represents the 3 documnets (0.0,0.1,0.2)\r\n",
        "sentenceData = spark.createDataFrame([\r\n",
        "        (0.0, \"Welcome to KDM TF_IDF Tutorial.\"),\r\n",
        "        (0.1, \"Learn Spark ml tf_idf in today's lab.\"),\r\n",
        "        (0.2, \"Spark Mllib has TF-IDF.\")\r\n",
        "    ], [\"label\", \"sentence\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1pVrcFAHydd"
      },
      "source": [
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(sentenceData)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3li0lw5IB95",
        "outputId": "700da80c-7d16-425e-c772-6d4df05f78ab"
      },
      "source": [
        "wordsData.show()\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Welcome to KDM TF...|[welcome, to, kdm...|\n",
            "|  0.1|Learn Spark ml tf...|[learn, spark, ml...|\n",
            "|  0.2|Spark Mllib has T...|[spark, mllib, ha...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgrh2KrnIa5k"
      },
      "source": [
        "# applying tf on the words data\\r\\n\",\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\r\n",
        "featurizedData = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kPkDf6II6X4"
      },
      "source": [
        "# calculating the IDF\\r\\n\",\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idfModel = idf.fit(featurizedData)\r\n",
        "rescaledData = idfModel.transform(featurizedData)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peLJWSwgJLDL",
        "outputId": "75782e7b-9170-486b-f170-6d97426d4625"
      },
      "source": [
        "#displaying the results\\r\\n\",\r\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[2,8,13,15,17...|\n",
            "|  0.1|(20,[2,3,6,7],[0....|\n",
            "|  0.2|(20,[6,14,15],[0....|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIWUwfC6Jj7G"
      },
      "source": [
        "spark2 = SparkSession.builder.appName(\"Ngram Example\").getOrCreate()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I302vGx-KmND"
      },
      "source": [
        "#creating dataframe of input\r\n",
        "wordDataFrame = spark2.createDataFrame([\r\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\r\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\r\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\r\n",
        "], [\"id\", \"words\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTk_f2b_KpDp"
      },
      "source": [
        "#creating NGrams with n=2 (two words)\\r\\n\",\r\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\r\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7W6NJewK4U-",
        "outputId": "5e6c5d6f-fd25-4ea2-d3ae-845707de2017"
      },
      "source": [
        "# displaying the results\\r\\n\",\r\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|ngrams                                                            |\n",
            "+------------------------------------------------------------------+\n",
            "|[Hi I, I heard, heard about, about Spark]                         |\n",
            "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
            "|[Logistic regression, regression models, models are, are neat]    |\n",
            "+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpqM00euLDJp"
      },
      "source": [
        "# creating spark session\\r\\n\",\r\n",
        "spark3 = SparkSession.builder.appName(\"Word2Vec Example\").getOrCreate()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqRPsFvoLvG8"
      },
      "source": [
        "# Input data: Each row is a bag of words from a sentence or document.\r\n",
        "documentDF = spark3.createDataFrame([\r\n",
        "(\"McCarthy was asked to analyse the data from the first phase of trials of the vaccine.\".split(\" \"), ),\r\n",
        "(\"We have amassed the raw data and are about to begin analysing it.\".split(\" \"), ),\r\n",
        "(\"Without more data we cannot make a meaningful comparison of the two systems.\".split(\" \"), ),\r\n",
        "(\"Collecting data is a painfully slow process.\".split(\" \"), ),\r\n",
        "(\"You need a long series of data to be able to discern such a trend.\".split(\" \"), )\r\n",
        "], [\"text\"])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufpoV0SFMlWe"
      },
      "source": [
        "# Learn a mapping from words to Vectors.\r\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\r\n",
        "model = word2Vec.fit(documentDF)\r\n",
        "result = model.transform(documentDF)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlsergebM4u-",
        "outputId": "9e4b56e0-4668-46a7-9e5d-ee044c532042"
      },
      "source": [
        "for row in result.collect():\r\n",
        "    text, vector = row\r\n",
        "    #printing the results\r\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: [McCarthy, was, asked, to, analyse, the, data, from, the, first, phase, of, trials, of, the, vaccine.] => \n",
            "Vector: [0.021903225395362824,0.011923420010134578,0.02301490874378942]\n",
            "\n",
            "Text: [We, have, amassed, the, raw, data, and, are, about, to, begin, analysing, it.] => \n",
            "Vector: [0.06271842372818635,-0.03274114481665982,0.002357496091952691]\n",
            "\n",
            "Text: [Without, more, data, we, cannot, make, a, meaningful, comparison, of, the, two, systems.] => \n",
            "Vector: [-0.005007334268436982,0.022124015761969183,-0.029689349377384554]\n",
            "\n",
            "Text: [Collecting, data, is, a, painfully, slow, process.] => \n",
            "Vector: [-0.021147702554506913,-0.047306668013334274,0.04175957611628941]\n",
            "\n",
            "Text: [You, need, a, long, series, of, data, to, be, able, to, discern, such, a, trend.] => \n",
            "Vector: [-0.04790672076245149,0.030444954708218574,-0.021157699078321456]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nvXfPr1NBCK",
        "outputId": "69fcbd5a-8d21-4009-b2a7-eb6e8ff646a1"
      },
      "source": [
        "# showing the synonyms and cosine similarity of the word in input data\r\n",
        "synonyms = model.findSynonyms(\"data\", 5)   # its okay for certain words , real bad for others\r\n",
        "synonyms.show(5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------------+\n",
            "|      word|        similarity|\n",
            "+----------+------------------+\n",
            "|       was|0.8477357625961304|\n",
            "|    series| 0.842510461807251|\n",
            "|   analyse|0.7764721512794495|\n",
            "|       are|0.7724130749702454|\n",
            "|meaningful|0.7415226697921753|\n",
            "+----------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "472ufcOpNh57"
      },
      "source": [
        "#closing the spark sessions\r\n",
        "spark.stop()\r\n",
        "spark2.stop()\r\n",
        "spark3.stop()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn9aP7K3ObNh"
      },
      "source": [
        "# Creating 5 separate text files containing text data (blogs,news articles etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwDFvB0wNlA0"
      },
      "source": [
        "with open(\"/txt1.txt\",\"r+\") as t1:\r\n",
        "  text1 = t1.read()\r\n",
        "with open(\"/txt2.txt\",\"r+\") as t2:\r\n",
        "  text2 = t2.read()\r\n",
        "with open(\"/txt3.txt\",\"r+\") as t3:\r\n",
        "  text3 = t3.read()\r\n",
        "with open(\"/txt4.txt\",\"r+\") as t4:\r\n",
        "  text4 = t4.read()\r\n",
        "with open(\"/txt5.txt\",\"r+\") as t5:\r\n",
        "  text5 = t5.read()\r\n",
        "# Read all 5 txt files which contains news articles\r\n",
        "documents = [text1,text2,text3,text4,text5]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRh733mLU4hE"
      },
      "source": [
        "# 1.Find out the top10 TF-IDF words for the above input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8_81frBUuD8",
        "outputId": "598ef602-8efc-4f8d-971e-ed14fec242b6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\\r\\n\",\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame\r\n",
        "pd.set_option('display.max_columns', 20)\r\n",
        "df.loc['Total'] = df.sum() \r\n",
        "# adding row to value total\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\\r\\n\",\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         school    bricks       our        my     salad   rabbits       for  \\\n",
            "0      0.000000  0.000000  0.000000  0.534522  0.000000  0.000000  0.000000   \n",
            "1      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2      0.404907  0.404907  0.404907  0.000000  0.000000  0.000000  0.000000   \n",
            "3      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "4      0.000000  0.000000  0.000000  0.000000  0.538498  0.538498  0.538498   \n",
            "Total  0.404907  0.404907  0.404907  0.534522  0.538498  0.538498  0.538498   \n",
            "\n",
            "             of       the        is  \n",
            "0      0.000000  0.000000  0.000000  \n",
            "1      0.000000  0.222913  0.185038  \n",
            "2      0.326676  0.000000  0.271171  \n",
            "3      0.213691  0.427381  0.000000  \n",
            "4      0.000000  0.000000  0.360638  \n",
            "Total  0.540367  0.650295  0.816848  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vtBpzgvWdJy"
      },
      "source": [
        "        2.Find out the top10 TF-IDF words for the lemmatized input\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LQVU1rCWjAg",
        "outputId": "e5b8ba82-4613-4ec7-e38c-80d9826aac69"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "words1 = nltk.word_tokenize(text1)\r\n",
        "words2 = nltk.word_tokenize(text2)\r\n",
        "words3 = nltk.word_tokenize(text3)\r\n",
        "words4 = nltk.word_tokenize(text4)\r\n",
        "words5 = nltk.word_tokenize(text5)\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "documents = [lemmatized_document1,lemmatized_document2,lemmatized_document3,lemmatized_document4,lemmatized_document5]\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\\r\\n\",\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\\r\\n\",\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\\r\\n\",\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \\r\\n\",\r\n",
        "df.loc['Total'] = df.sum()\r\n",
        "# adding row to value total\\r\\n\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\\r\\n\",\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "         school     brick       our        my     salad    rabbit       for  \\\n",
            "0      0.000000  0.000000  0.000000  0.534522  0.000000  0.000000  0.000000   \n",
            "1      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2      0.404907  0.404907  0.404907  0.000000  0.000000  0.000000  0.000000   \n",
            "3      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "4      0.000000  0.000000  0.000000  0.000000  0.538498  0.538498  0.538498   \n",
            "Total  0.404907  0.404907  0.404907  0.534522  0.538498  0.538498  0.538498   \n",
            "\n",
            "             of       the        is  \n",
            "0      0.000000  0.000000  0.000000  \n",
            "1      0.000000  0.222913  0.185038  \n",
            "2      0.326676  0.000000  0.271171  \n",
            "3      0.213691  0.427381  0.000000  \n",
            "4      0.000000  0.000000  0.360638  \n",
            "Total  0.540367  0.650295  0.816848  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHqQwlZrYttY"
      },
      "source": [
        "# 3.Find out the top10TF-IDF words for the n-gram based input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iewzjmRHY3Zj",
        "outputId": "644671bb-bc74-41ed-ed52-b87a428caabd"
      },
      "source": [
        "# this function takes document and n int value to generate list of n grams\\r\\n\",\r\n",
        "def ngrams(input, n):\r\n",
        "  input = input.split(' ')\r\n",
        "  output = []\r\n",
        "  for i in range(len(input)-n+1):\r\n",
        "    output.append(input[i:i+n])\r\n",
        "    return output\r\n",
        "ngram_text1 = ' '.join([' '.join(x) for x in ngrams(text1, 3)])\r\n",
        "ngram_text2 = ' '.join([' '.join(x) for x in ngrams(text2, 3)])\r\n",
        "ngram_text3 = ' '.join([' '.join(x) for x in ngrams(text3, 3)])\r\n",
        "ngram_text4 = ' '.join([' '.join(x) for x in ngrams(text4, 3)])\r\n",
        "ngram_text5 = ' '.join([' '.join(x) for x in ngrams(text5, 3)])\r\n",
        "# document = [ngram_doc1,ngram_doc2,ngram_doc3,ngram_doc4,ngram_doc5]\\r\\n\",\r\n",
        "documents = [text1,text2,text3,text4,text5]\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\\r\\n\",\r\n",
        "vect = TfidfVectorizer( ngram_range=(3,3))\r\n",
        " # TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams\\r\\n\",\r\n",
        "#created TfidfVectorizer object\\r\\n\",\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\\r\\n\",\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \\r\\n\",\r\n",
        "df.loc['Total'] = df.sum() \r\n",
        "# adding row to value total\\r\\n\",\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\\r\\n\",\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\\r\\n\",\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       me to finish  my plate at  mom taught me  our school building  \\\n",
            "0          0.316228     0.316228       0.316228             0.000000   \n",
            "1          0.000000     0.000000       0.000000             0.000000   \n",
            "2          0.000000     0.000000       0.000000             0.447214   \n",
            "3          0.000000     0.000000       0.000000             0.000000   \n",
            "4          0.000000     0.000000       0.000000             0.000000   \n",
            "Total      0.316228     0.316228       0.316228             0.447214   \n",
            "\n",
            "       is made of  made of bricks  building is made  school building is  \\\n",
            "0        0.000000        0.000000          0.000000            0.000000   \n",
            "1        0.000000        0.000000          0.000000            0.000000   \n",
            "2        0.447214        0.447214          0.447214            0.447214   \n",
            "3        0.000000        0.000000          0.000000            0.000000   \n",
            "4        0.000000        0.000000          0.000000            0.000000   \n",
            "Total    0.447214        0.447214          0.447214            0.447214   \n",
            "\n",
            "       salad is for  is for rabbits  \n",
            "0          0.000000        0.000000  \n",
            "1          0.000000        0.000000  \n",
            "2          0.000000        0.000000  \n",
            "3          0.000000        0.000000  \n",
            "4          0.707107        0.707107  \n",
            "Total      0.707107        0.707107  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSPQBX0ijFZx"
      },
      "source": [
        "2.Write a simple spark program to read a dataset and find the W2V similar words (words with higher cosine similarity) for the Top10 TF-IDF Words\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfGmq00wjJf6",
        "outputId": "542fd4b3-36f7-48bf-88d8-6956cca91b29"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec\r\n",
        "# creating spark session\\r\\n\",\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, text1),\r\n",
        "        (0.1, text2),\r\n",
        "        (0.2, text3),\r\n",
        "        (0.3, text4),\r\n",
        "        (0.5, text5)\r\n",
        "        ], [\"label\", \"document\"])\r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|My mom taught me ...|[my, mom, taught,...|\n",
            "|  0.1|The only problem ...|[the, only, probl...|\n",
            "|  0.2|Our school buildi...|[our, school, bui...|\n",
            "|  0.3|Every night I get...|[every, night, i,...|\n",
            "|  0.5|Salad is for rabb...|[salad, is, for, ...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pui3BjuchHdZ"
      },
      "source": [
        "Try without NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxXW4D4UhuEA",
        "outputId": "a9aa077d-fd6d-4c14-a1bd-dd8790378f30"
      },
      "source": [
        "# applying tf on the words data\\r\\n\",\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\\r\\n\",\r\n",
        "# calculating the IDF\\r\\n\",\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\\r\\n\",\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "print(\"TF-IDF without NLP:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[28,40,55,88...|\n",
            "|  0.1|(200,[5,9,17,26,2...|\n",
            "|  0.2|(200,[9,28,52,79,...|\n",
            "|  0.3|(200,[17,26,28,32...|\n",
            "|  0.5|(200,[9,28,115,14...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF without NLP:\n",
            "Row(label=0.0, document='My mom taught me to finish everything on my plate at dinner .', words=['my', 'mom', 'taught', 'me', 'to', 'finish', 'everything', 'on', 'my', 'plate', 'at', 'dinner', '.'], rawFeatures=SparseVector(200, {28: 1.0, 40: 1.0, 55: 1.0, 88: 1.0, 125: 1.0, 133: 1.0, 136: 1.0, 156: 1.0, 162: 1.0, 168: 2.0, 169: 1.0, 178: 1.0}), features=SparseVector(200, {28: 0.0, 40: 1.0986, 55: 1.0986, 88: 1.0986, 125: 1.0986, 133: 1.0986, 136: 1.0986, 156: 0.6931, 162: 1.0986, 168: 2.1972, 169: 1.0986, 178: 1.0986}))\n",
            "(200,[28,40,55,88,125,133,136,156,162,168,169,178],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])\n",
            "Row(label=0.1, document='The only problem with a pencil , is that they do not stay sharp long enough .', words=['the', 'only', 'problem', 'with', 'a', 'pencil', ',', 'is', 'that', 'they', 'do', 'not', 'stay', 'sharp', 'long', 'enough', '.'], rawFeatures=SparseVector(200, {5: 1.0, 9: 1.0, 17: 1.0, 26: 1.0, 28: 1.0, 33: 1.0, 45: 1.0, 48: 1.0, 50: 1.0, 67: 2.0, 98: 1.0, 99: 1.0, 104: 1.0, 148: 2.0, 160: 1.0}), features=SparseVector(200, {5: 1.0986, 9: 0.4055, 17: 0.6931, 26: 0.6931, 28: 0.0, 33: 1.0986, 45: 1.0986, 48: 1.0986, 50: 1.0986, 67: 1.3863, 98: 1.0986, 99: 1.0986, 104: 1.0986, 148: 2.1972, 160: 1.0986}))\n",
            "(200,[5,9,17,26,28,33,45,48,50,67,98,99,104,148,160],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0])\n",
            "Row(label=0.2, document='Our school building is made of brick .', words=['our', 'school', 'building', 'is', 'made', 'of', 'brick', '.'], rawFeatures=SparseVector(200, {9: 1.0, 28: 1.0, 52: 1.0, 79: 1.0, 95: 1.0, 144: 1.0, 165: 1.0, 194: 1.0}), features=SparseVector(200, {9: 0.4055, 28: 0.0, 52: 1.0986, 79: 1.0986, 95: 0.6931, 144: 0.6931, 165: 1.0986, 194: 1.0986}))\n",
            "(200,[9,28,52,79,95,144,165,194],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document='Every night I get woken up by the sound of a barking dog across the street .', words=['every', 'night', 'i', 'get', 'woken', 'up', 'by', 'the', 'sound', 'of', 'a', 'barking', 'dog', 'across', 'the', 'street', '.'], rawFeatures=SparseVector(200, {17: 2.0, 26: 1.0, 28: 1.0, 32: 1.0, 67: 1.0, 69: 1.0, 74: 1.0, 81: 1.0, 84: 1.0, 95: 1.0, 151: 1.0, 156: 1.0, 159: 1.0, 164: 1.0, 166: 1.0, 181: 1.0}), features=SparseVector(200, {17: 1.3863, 26: 0.6931, 28: 0.0, 32: 1.0986, 67: 0.6931, 69: 1.0986, 74: 1.0986, 81: 1.0986, 84: 1.0986, 95: 0.6931, 151: 1.0986, 156: 0.6931, 159: 1.0986, 164: 1.0986, 166: 1.0986, 181: 1.0986}))\n",
            "(200,[17,26,28,32,67,69,74,81,84,95,151,156,159,164,166,181],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document='Salad is for rabbit .', words=['salad', 'is', 'for', 'rabbit', '.'], rawFeatures=SparseVector(200, {9: 1.0, 28: 1.0, 115: 1.0, 144: 1.0, 161: 1.0}), features=SparseVector(200, {9: 0.4055, 28: 0.0, 115: 1.0986, 144: 0.6931, 161: 1.0986}))\n",
            "(200,[9,28,115,144,161],[1.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYPc5dWelNf6"
      },
      "source": [
        "Try with Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OGwxdHXi4a0",
        "outputId": "072720b6-785d-4c2e-ea52-e1576f6352f7"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "words1 = nltk.word_tokenize(text1)\r\n",
        "words2 = nltk.word_tokenize(text2)\r\n",
        "words3 = nltk.word_tokenize(text3)\r\n",
        "words4 = nltk.word_tokenize(text4)\r\n",
        "words5 = nltk.word_tokenize(text5)\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "### lemmatizing words from 5 input docs same as previos task\\r\\n\",\r\n",
        "# creating spark session\\r\\n\",\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, lemmatized_document1),\r\n",
        "        (0.1, lemmatized_document2),\r\n",
        "        (0.2, lemmatized_document3),\r\n",
        "        (0.3, lemmatized_document4),\r\n",
        "        (0.5, lemmatized_document5)\r\n",
        "            ], [\"label\", \"document\"])\r\n",
        "        # creating tokens/words from the sentence data\\r\\n\",\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|My mom taught me ...|[my, mom, taught,...|\n",
            "|  0.1|The only problem ...|[the, only, probl...|\n",
            "|  0.2|Our school buildi...|[our, school, bui...|\n",
            "|  0.3|Every night I get...|[every, night, i,...|\n",
            "|  0.5|Salad is for rabb...|[salad, is, for, ...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woXGCO42p8Hh",
        "outputId": "fcd3e84a-6d27-4e17-b9e1-8a93add8f54b"
      },
      "source": [
        "# applying tf on the words data\\r\\n\",\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\\r\\n\",\r\n",
        "# calculating the IDF\\r\\n\",\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\\r\\n\",\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "print(\"TF-IDF with Lemmatization:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "    spark.stop()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[28,40,55,88...|\n",
            "|  0.1|(200,[5,9,17,26,2...|\n",
            "|  0.2|(200,[9,28,52,79,...|\n",
            "|  0.3|(200,[17,26,28,32...|\n",
            "|  0.5|(200,[9,28,115,14...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with Lemmatization:\n",
            "Row(label=0.0, document='My mom taught me to finish everything on my plate at dinner .', words=['my', 'mom', 'taught', 'me', 'to', 'finish', 'everything', 'on', 'my', 'plate', 'at', 'dinner', '.'], rawFeatures=SparseVector(200, {28: 1.0, 40: 1.0, 55: 1.0, 88: 1.0, 125: 1.0, 133: 1.0, 136: 1.0, 156: 1.0, 162: 1.0, 168: 2.0, 169: 1.0, 178: 1.0}), features=SparseVector(200, {28: 0.0, 40: 1.0986, 55: 1.0986, 88: 1.0986, 125: 1.0986, 133: 1.0986, 136: 1.0986, 156: 0.6931, 162: 1.0986, 168: 2.1972, 169: 1.0986, 178: 1.0986}))\n",
            "(200,[28,40,55,88,125,133,136,156,162,168,169,178],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])\n",
            "Row(label=0.1, document='The only problem with a pencil , is that they do not stay sharp long enough .', words=['the', 'only', 'problem', 'with', 'a', 'pencil', ',', 'is', 'that', 'they', 'do', 'not', 'stay', 'sharp', 'long', 'enough', '.'], rawFeatures=SparseVector(200, {5: 1.0, 9: 1.0, 17: 1.0, 26: 1.0, 28: 1.0, 33: 1.0, 45: 1.0, 48: 1.0, 50: 1.0, 67: 2.0, 98: 1.0, 99: 1.0, 104: 1.0, 148: 2.0, 160: 1.0}), features=SparseVector(200, {5: 1.0986, 9: 0.4055, 17: 0.6931, 26: 0.6931, 28: 0.0, 33: 1.0986, 45: 1.0986, 48: 1.0986, 50: 1.0986, 67: 1.3863, 98: 1.0986, 99: 1.0986, 104: 1.0986, 148: 2.1972, 160: 1.0986}))\n",
            "(200,[5,9,17,26,28,33,45,48,50,67,98,99,104,148,160],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0])\n",
            "Row(label=0.2, document='Our school building is made of brick .', words=['our', 'school', 'building', 'is', 'made', 'of', 'brick', '.'], rawFeatures=SparseVector(200, {9: 1.0, 28: 1.0, 52: 1.0, 79: 1.0, 95: 1.0, 144: 1.0, 165: 1.0, 194: 1.0}), features=SparseVector(200, {9: 0.4055, 28: 0.0, 52: 1.0986, 79: 1.0986, 95: 0.6931, 144: 0.6931, 165: 1.0986, 194: 1.0986}))\n",
            "(200,[9,28,52,79,95,144,165,194],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document='Every night I get woken up by the sound of a barking dog across the street .', words=['every', 'night', 'i', 'get', 'woken', 'up', 'by', 'the', 'sound', 'of', 'a', 'barking', 'dog', 'across', 'the', 'street', '.'], rawFeatures=SparseVector(200, {17: 2.0, 26: 1.0, 28: 1.0, 32: 1.0, 67: 1.0, 69: 1.0, 74: 1.0, 81: 1.0, 84: 1.0, 95: 1.0, 151: 1.0, 156: 1.0, 159: 1.0, 164: 1.0, 166: 1.0, 181: 1.0}), features=SparseVector(200, {17: 1.3863, 26: 0.6931, 28: 0.0, 32: 1.0986, 67: 0.6931, 69: 1.0986, 74: 1.0986, 81: 1.0986, 84: 1.0986, 95: 0.6931, 151: 1.0986, 156: 0.6931, 159: 1.0986, 164: 1.0986, 166: 1.0986, 181: 1.0986}))\n",
            "(200,[17,26,28,32,67,69,74,81,84,95,151,156,159,164,166,181],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document='Salad is for rabbit .', words=['salad', 'is', 'for', 'rabbit', '.'], rawFeatures=SparseVector(200, {9: 1.0, 28: 1.0, 115: 1.0, 144: 1.0, 161: 1.0}), features=SparseVector(200, {9: 0.4055, 28: 0.0, 115: 1.0986, 144: 0.6931, 161: 1.0986}))\n",
            "(200,[9,28,115,144,161],[1.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}